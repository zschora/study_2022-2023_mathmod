---
## Front matter
title: "Доклад"
subtitle: "Линейные модели"
author: "Шалыгин Георгий Эдуардович"

## Generic otions
lang: ru-RU
toc-title: "Содержание"

## Bibliography
bibliography: bib/cite.bib
csl: pandoc/csl/gost-r-7-0-5-2008-numeric.csl

## Pdf output format
toc: true # Table of contents
toc-depth: 2
lof: true # List of figures
lot: true # List of tables
fontsize: 12pt
linestretch: 1.5
papersize: a4
documentclass: scrreprt
## I18n polyglossia
polyglossia-lang:
  name: russian
  options:
	- spelling=modern
	- babelshorthands=true
polyglossia-otherlangs:
  name: english
## I18n babel
babel-lang: russian
babel-otherlangs: english
## Fonts
mainfont: PT Serif
romanfont: PT Serif
sansfont: PT Sans
monofont: PT Mono
mainfontoptions: Ligatures=TeX
romanfontoptions: Ligatures=TeX
sansfontoptions: Ligatures=TeX,Scale=MatchLowercase
monofontoptions: Scale=MatchLowercase,Scale=0.9
## Biblatex
biblatex: true
biblio-style: "gost-numeric"
biblatexoptions:
  - parentracker=true
  - backend=biber
  - hyperref=auto
  - language=auto
  - autolang=other*
  - citestyle=gost-numeric
## Pandoc-crossref LaTeX customization
figureTitle: "Рис."
tableTitle: "Таблица"
listingTitle: "Листинг"
lofTitle: "Список иллюстраций"
lotTitle: "Список таблиц"
lolTitle: "Листинги"
## Misc options
indent: true
header-includes:
  - \usepackage{indentfirst}
  - \usepackage{float} # keep figures where there are in the text
  - \floatplacement{figure}{H} # keep figures where there are in the text
---

# Цели и задачи

- Изучить линейные модели.
- Задачи:
  - Рассмотреть задачи, решаемые линейными моделями.
  - Изучить линейные модели и варианты их применения в задачах. 
  - Исследовать методы улучшения линейных моделей.

# Задачи

## Регрессия

### Постановка задачи

Задача контролируемого машинного обучения, которая прогнозирует значение метки по набору связанных компонентов. Метка здесь может принимать любое значение, а не просто выбирается из конечного набора значений, как в задачах классификации. Алгоритмы регрессии моделируют зависимость меток от связанных компонентов, чтобы определить закономерности изменения меток при разных значениях компонентов. На вход алгоритма регрессии подается набор примеров с метками известных значений. Результатом работы алгоритма регрессии является функция, которая умеет прогнозировать значения метки для любого нового набора входных компонентов. Вот несколько примеров для сценария регрессии:
- прогнозирование цен на дома по таким атрибутам, как количество комнат, расположение и размер;
- прогнозирование будущей цены акций на основе исторических данных и текущих тенденций рынка;
- прогнозирование продаж товара в зависимости от рекламного бюджета.
В итоге, рассматривается регрессионная модель зависимости одной (объясняемой, зависимой) переменной $y$ от другой или нескольких других переменных (факторов, регрессоров, независимых переменных) ${x_i}$ с линейной функцией зависимости: $M: \mathbb{X}  \rightarrow \mathbb{R}$.
Подробнее в [@re].

### Модель для задачи регрессии
По набору $x_1, x_2, ..., x_d$ предсказываем $y$.
Модель описывается формулоой $$\widetilde{y} = w_1x_1+...+w_dx_d + w_0$$ для какого-то фиксированного набора ${w_i}$.
Изначально, предполагаем, что $\forall y_i: y_i = \widetilde{y} + \epsilon$, где ошибка $\epsilon$ описывается нормальным распределением.
Пример регрессии для двух переменных приведен на @fig:001.

![График регрессии](image\lr.png){#fig:001}

## Классификация

### Постановка задачи

Задача контролируемого машинного обучения, которая прогнозирует распределение элементов данных по двум классам (категориям). На вход алгоритма классификации подается набор примеров с метками, каждая из которых представляет собой целое число 0 или 1. Результатом работы алгоритма двоичной классификации является классификатор, который умеет прогнозировать класс для новых экземпляров без метки. Вот несколько примеров для сценария двоичной классификации:

- Распределение комментариев Twitter по тональности — позитивные или негативные.
- Диагностика пациента на наличие определенной болезни.
- Принятие решений о присвоении отметки "спам" сообщению электронной почты.
- Определение того, содержит ли фотография определенный элемент, например изображение собаки или фрукта.

Для получения наилучших результатов обучения двоичной классификации обучающие данные должны быть сбалансированы (т. е. число положительных и отрицательных обучающих данных должно быть одинаковым). Отсутствующие значения необходимо обработать до обучения.
Дополнительные сведения см. в [@cl].
### Модель для задачи классификации

Формально, строится отображение $M: \mathbb{X}  \rightarrow 1, 2, ..., k$, $k$ -- количество классов. Рассмотрим самый распространенный случай $k=2$.
Для решения проблемы задача регрессии может быть сформулирована иначе: вместо предсказания бинарной переменной мы предсказываем непрерывную переменную со значениями на отрезке $[0,1]$ при любых значениях независимых переменных. Это достигается применением следующего регрессионного уравнения $sigm(\widetilde{y})$, где $sigm(t) = \dfrac{1}{1+e^{-t}}$. Функция приведена на @fig:002.

![Функция sigm](image\logreg.png){#fig:002}

Данный вид классификации называется логистической регрессией.



# Обучение линейных моделей.

## Метод наименьших квадратов для линейной регрессии
Метод наименьших квадратов (МНК) — математический метод, применяемый для решения различных задач, основанный на минимизации суммы квадратов отклонений некоторых функций от экспериментальных входных данных. Он может использоваться для «решения» переопределенных систем уравнений (когда количество уравнений превышает количество неизвестных), для поиска решения в случае обычных (не переопределенных) нелинейных систем уравнений, для аппроксимации точечных значений некоторой функции. МНК является одним из базовых методов регрессионного анализа для оценки неизвестных параметров регрессионных моделей по выборочным данным.
Сущность МНК (обычного, классического) заключается в том, чтобы найти такие параметры ${w_i}$, при которых сумма квадратов отклонений (ошибок, для регрессионных моделей их часто называют остатками регрессии) $MSE = (\widetilde{y} - y)^2$ будет минимальной.

### Аналитический подход

Минимум функции находится с помощью градиента.
Получившееся решение имеет вид:
$$w = (X^TX)^{-1}X^Ty$$

### Градиентный спуск
Градиентный спуск — это алгоритм оптимизации, используемый для минимизации ошибок в модели машинного обучения. Он работает путем итеративной корректировки параметров модели в направлении отрицательного градиента функции потерь (которая представляет ошибку), чтобы уменьшить ошибку и найти оптимальные параметры, которые дают наилучшие результаты прогнозирования. Алгоритм продолжает этот процесс до тех пор, пока он не достигнет минимума или не будет выполнен заранее определенный критерий остановки.
Описывается формулой: $$w_j = w_j - \alpha\dfrac{d}{dw_j}MSE$$
Код алгоритма:
```python
# Batch Gradient Descent
import numpy as np
eta = 0.1 # learning rate
n_iterations = 1000
m = 100

X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)
X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance
theta = np.random.randn(2,1) #random initilization
for iteration in range(n_iterations):
    gradients = 2/m * X_b.T.dot(X_b.dot(theta)-y)
    theta = theta - eta * gradients
print(theta)
```
Подробнее о методах обучения и реализации моделей в [@ob].

# Регуляриза

## Обучение логистической регрессии для бинарной классификации

Задача обучения линейного классификатора заключается в том, чтобы по выборке $X$ настроить вектор весов $w$.
В логистической регрессии для этого решается задача минимизации эмпирического риска с функцией потерь специального вида:
$$Q(w)=\sum\limits_{i=1}^m\ln(1+exp(-y_i\left\langle x_i,w\right\rangle))\rightarrow min_w$$
Задача минимизации решается любыми вычислительными методами, напрмер, описанным выше градиентным спуском.

Подробнее о методах обучения и реализации моделей в [@ob].

# Регуляризация
## Описание
Регуляризация (англ. regularization) в статистике, машинном обучении, теории обратных задач — метод добавления некоторых дополнительных ограничений к условию с целью решить некорректно поставленную задачу или предотвратить переобучение. Чаще всего эта информация имеет вид штрафа за сложность модели.
Однин из способов бороться с негативным эффектом излишнего подстраивания под данные — использование регуляризации, т. е. добавление некоторого штрафа за большие значения коэффициентов у линейной модели. Тем самым запрещаются слишком "резкие" изгибы, и предотвращается переобучение.
## Виды регуляризации
Переобучение в большинстве случаев проявляется в том, что итоговые модели имеют слишком большие значения параметров. Соответственно, необходимо добавить в целевую функцию штраф за это. Наиболее часто используемые виды регуляризации — $L_1$ и $L_2$, а также их линейная комбинация — эластичная сеть.
- $L_1$: $$Q(w, X) = \lambda\sum\limits_{j=1}^nw_j^2$$
  Минимизация регуляризованного cоответствующим образом эмпирического риска приводит к выбору такого вектора параметров $w$, которое не слишком сильно отклоняется от нуля. В линейных классификаторах это позволяет избежать проблем мультиколлинеарности и переобучения.
- $L_2$: $$Q(w, X) = \lambda\sum\limits_{j=1}^n|w_j|$$
  Данный вид регуляризации также позволяет ограничить значения вектора $w$. Однако, к тому же он обладает интересным и полезным на практике свойством — обнуляет значения некоторых параметров, что в случае с линейными моделями приводит к отбору признаков.
  Для задачи линейной регрессии в этом случае существует аналитическое решение в виде $$w = (X^TX+\lambda I)^{-1}X^Ty$$

Дальнейшие сведения можно получить в [@vor].

# Выводы

В итоге были рассмотрены линейные модели, имеющие простой вид, высокий уровень интерпретируемости, множество алгоритмов построения и применяющиеся в огромном числе задачах, сводимых к классификации и регрессии.

# Список литературы{.unnumbered}

::: {#refs}
:::
