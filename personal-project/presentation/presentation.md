---
## Front matter
lang: ru-RU
title: Доклад
subtitle: Линейные модели
author:
  - Шалыгин Г. Э.
institute:
  - Российский университет дружбы народов, Москва, Россия
date:

## i18n babel
babel-lang: russian
babel-otherlangs: english

## Formatting pdf
toc: false
toc-title: Содержание
slide_level: 2
aspectratio: 169
section-titles: true
theme: metropolis
header-includes:
 - \metroset{progressbar=frametitle,sectionpage=progressbar,numbering=fraction}
 - '\makeatletter'
 - '\beamer@ignorenonframefalse'
 - '\makeatother'
---

# Информация

## Докладчик

  * Шалыгин Георгий Эдуардович
  * студент НФИ-02-20
  * Российский университет дружбы народов

# Вводная часть

## Актуальность

- Предсказание значений признаков, нахождения зависимостей в данных.
- Регрессия, классификация.
- Простота, высокий уровень интерпретируемости, множество алгоритмов построения (обучения)

## Цели и задачи

- Изучить линейные модели.
- Задачи:
  - Рассмотреть задачи, решаемые линейными моделями.
  - Изучить линейные модели и варианты их применения в задачах. 
  - Исследовать методы улучшения линейных моделей.

# Задачи

## Регрессия

- Предсказание стоимости квартиры.
- (площадь, этаж, число комнат) -> стоимость
- $M: \mathbb{X}  \rightarrow \mathbb{R}$

## Классификация

- Есть набор операций по банковской карте, а вы бы хотели, понять, какие из этих операций сделали мошенники.
- $M: \mathbb{X}  \rightarrow 1, 2, ..., k$, $k$ -- количество классов.

# Модели

## Регрессия

- По набору $x_1, x_2, ..., x_d$ предсказываем $y$.
- Модель: $$\widetilde{y} = w_1x_1+...+w_dx_d + w_0$$ для какого-то набора ${w_i}$.
- Предполагаем, что $y_i \approx \widetilde{y}$.
- Уравнение гиперплоскости в пространстве размерности $d+1$.

## Двумерная регрессия

Если признак один, то это прямая:

![Линейная регрессия](D:\work\study\2022-2023\Матмод\study_2022-2023_mathmod\personal-project\presentation\image\lr.png){#fig:001 width=43%}

## Задача обучения

- Функция потерь: $MSE = \sum\limits_1^n(y - \widetilde{y})^2$.
- Задача минимизации $MSE \rightarrow min$.

## Возможные решения

- Аналитическое: $$w = (X^TX)^{-1}X^Ty$$
- Приближенное решение:
  - градиентный спуск $$w_j = w_j - \alpha\dfrac{d}{dw_j}MSE$$
  - стохастический градиентный спуск (считаем изменение по подвыборке из $X$).

## Классификация

- Есть набор операций по банковской карте, а вы бы хотели, понять, какие из этих операций сделали мошенники.

- $M: \mathbb{X}  \rightarrow 1, 2, ..., k$, $k$ -- количество классов.

- Модель: $sigm(\widetilde{y})$.

- Задача минимизировать кол-во ошибок.

  

![Функция sigm](D:\work\study\2022-2023\Матмод\study_2022-2023_mathmod\personal-project\presentation\image\logreg.png){#fig:002 width=35%}

# Дальнейшее изучение моделей

## Возможности усовершенствования

- Интерпретация: Цена = 10 х площадь + 1.1 х этаж + 20 х (число комнат).

- Оценка вклада признаков. Тогда признаки нормализуем, коэффициенты покажут их значимость.

- Кроме известных признаков, можно сгенерировать новые:

  Цена = 10 х площадь + 1.1 х этаж + 20 х число комнат − 0.2 х $\text{этаж}^2$ + 0.5 х площадь х число комнат + ⋯

## Регуляризация

- Признаки линейно зависимы (матрица необратима, коэффициенты не показательны, большая погрешность).
- Изменим функция минимизации: $$min(MSE + \lambda |w|^k)$$.
- Для $L^2$ аналитическое решение: $$w = (X^TX+\lambda I)^{-1}X^Ty$$

# Вывод

В итоге были рассмотрены линейные модели, имеющие простой вид, высокий уровень интерпретируемости, множество алгоритмов построения и применяющиеся в огромном числе задачах, сводимых к классификации и регрессии.
